{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FS8FdkGqQdF3",
        "outputId": "d2817462-0ad1-4bd4-b5db-aac264a18e0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'neuralcoref'...\n",
            "remote: Enumerating objects: 772, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 772 (delta 10), reused 16 (delta 7), pack-reused 748\u001b[K\n",
            "Receiving objects: 100% (772/772), 67.85 MiB | 20.14 MiB/s, done.\n",
            "Resolving deltas: 100% (407/407), done.\n",
            "Checking out files: 100% (151/151), done.\n",
            "/content/neuralcoref\n",
            "Requirement already satisfied: spacy<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (2.2.4)\n",
            "Requirement already satisfied: cython>=0.25 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.29.30)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (3.6.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (3.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (0.9.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (1.21.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (1.0.7)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (4.64.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (2.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (4.2.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 3)) (0.7.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 3)) (21.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 3)) (1.11.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 3)) (8.13.0)\n",
            "Obtaining file:///content/neuralcoref\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref==4.0) (1.21.6)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.23.5-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref==4.0) (2.23.0)\n",
            "Requirement already satisfied: spacy<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref==4.0) (2.2.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (2.10)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (3.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (0.9.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (1.0.7)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (57.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (4.64.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (2.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (3.8.0)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.6 MB/s \n",
            "\u001b[?25hCollecting botocore<1.27.0,>=1.26.5\n",
            "  Downloading botocore-1.26.5-py3-none-any.whl (8.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.8 MB 48.4 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.27.0,>=1.26.5->boto3->neuralcoref==4.0) (2.8.2)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 59.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.27.0,>=1.26.5->boto3->neuralcoref==4.0) (1.15.0)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, neuralcoref\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Running setup.py develop for neuralcoref\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.23.5 botocore-1.26.5 jmespath-1.0.0 neuralcoref-4.0 s3transfer-0.5.2 urllib3-1.25.11\n"
          ]
        }
      ],
      "source": [
        "# !pip install neuralcoref \n",
        "!git clone https://github.com/huggingface/neuralcoref.git\n",
        "%cd neuralcoref\n",
        "!pip install -r requirements.txt\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4t71nh92QwBS"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAoBk6vFQ9TF",
        "outputId": "83971756-5a62-40f6-f641-da76fd75532b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40155833/40155833 [00:00<00:00, 46822433.35B/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<spacy.lang.en.English at 0x7f26a7711410>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import neuralcoref\n",
        "neuralcoref.add_to_pipe(nlp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ym7zOYPGI5e5",
        "outputId": "f1a44b90-9433-4b18-9a3b-07be9c485173"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLUbncIZ6hIO",
        "outputId": "401307a2-2987-49fc-e825-24b5b9e1b422"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "file_path = \"/content/01.txt\"\n",
        "f = open(file_path, \"r\")\n",
        "s = f.read()\n",
        "temp = sent_tokenize(s)\n",
        "\n",
        "# print(temp[100:105])\n",
        "\n",
        "\n",
        "# # print(len(temp))\n",
        "\n",
        "# f = temp[100]\n",
        "# f = f.replace('\\n', ' ')\n",
        "\n",
        "# print(f)\n",
        "# print(type(f))\n",
        "\n",
        "# doc = nlp(f)\n",
        "# print()\n",
        "# print(doc)\n",
        "# resolved_doc = doc._.coref_resolved\n",
        "# print()\n",
        "# print(resolved_doc)\n",
        "\n",
        "# temp = []\n",
        "# book_list = [\"05\"]\n",
        "# sen_list = []\n",
        "\n",
        "# for i in book_list:\n",
        "#   book_path = file_path + i + \".txt\"\n",
        "#   #print(book_path)\n",
        "  \n",
        "#   f = open(book_path, \"r\")\n",
        "#   s = f.read()\n",
        "#   temp = sent_tokenize(s)\n",
        "#   print(temp[100])\n",
        "#   print(len(temp))\n",
        "#   # for x in temp:\n",
        "#   #   sen_list.append(x)\n",
        "# # print(sen_list[100])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPvEQQBZ9eLu"
      },
      "outputs": [],
      "source": [
        "resolved_sentences = [\" \"]\n",
        "for s in temp:\n",
        "  s = s.replace('\\n', ' ')\n",
        "  doc = nlp(s)\n",
        "  resolved_doc = doc._.coref_resolved\n",
        "  resolved_sentences.append(resolved_doc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIjZNTZs_6j3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "dict = {'sentence': resolved_sentences}  \n",
        "       \n",
        "df = pd.DataFrame(dict) \n",
        "    \n",
        "df.to_csv('chap-1.csv') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xs14vd2VI04p",
        "outputId": "ca9b42b3-52f1-43ed-d780-01066a7ed9a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "file_path = \"/content/02.txt\"\n",
        "f = open(file_path, \"r\")\n",
        "s = f.read()\n",
        "temp = sent_tokenize(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xur4VddzAObN"
      },
      "outputs": [],
      "source": [
        "resolved_sentences = []\n",
        "for s in temp:\n",
        "  s = s.replace('\\n', ' ')\n",
        "  # s = resolved_sentences[-1] + s\n",
        "  doc = nlp(s)\n",
        "  resolved_doc = doc._.coref_resolved\n",
        "  resolved_sentences.append(resolved_doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5OciKzqF86R"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "dict = {'sentence': resolved_sentences}  \n",
        "       \n",
        "df = pd.DataFrame(dict) \n",
        "    \n",
        "df.to_csv('chap-2.csv') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQc0m_tUGD1k",
        "outputId": "df715919-7213-42d9-9a43-d808473592f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "file_path = \"/content/03.txt\"\n",
        "f = open(file_path, \"r\")\n",
        "s = f.read()\n",
        "temp = sent_tokenize(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VztoNwUwJJjW"
      },
      "outputs": [],
      "source": [
        "resolved_sentences = []\n",
        "for s in temp:\n",
        "  s = s.replace('\\n', ' ')\n",
        "  doc = nlp(s)\n",
        "  resolved_doc = doc._.coref_resolved\n",
        "  resolved_sentences.append(resolved_doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WXEL8bWJJpO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "dict = {'sentence': resolved_sentences}  \n",
        "       \n",
        "df = pd.DataFrame(dict) \n",
        "    \n",
        "df.to_csv('chap-3.csv') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5Dgi2J6WcN6",
        "outputId": "2e560984-b962-410a-bad6-2b4d9f8db38e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "file_path = \"/content/04.txt\"\n",
        "f = open(file_path, \"r\")\n",
        "s = f.read()\n",
        "temp = sent_tokenize(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WsgII2KWcSV"
      },
      "outputs": [],
      "source": [
        "resolved_sentences = []\n",
        "for s in temp:\n",
        "  s = s.replace('\\n', ' ')\n",
        "  doc = nlp(s)\n",
        "  resolved_doc = doc._.coref_resolved\n",
        "  resolved_sentences.append(resolved_doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keRndwkxWcXA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "dict = {'sentence': resolved_sentences}  \n",
        "       \n",
        "df = pd.DataFrame(dict) \n",
        "    \n",
        "df.to_csv('chap-4.csv') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKP3NUxSLbV9",
        "outputId": "71a1226f-e4cc-4c6a-f508-5bf48a1faf86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "file_path = \"/content/12.txt\"\n",
        "f = open(file_path, \"r\")\n",
        "s = f.read()\n",
        "temp = sent_tokenize(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4h8VcPk5LqzV"
      },
      "outputs": [],
      "source": [
        "resolved_sentences = []\n",
        "for s in temp:\n",
        "  s = s.replace('\\n', ' ')\n",
        "  doc = nlp(s)\n",
        "  resolved_doc = doc._.coref_resolved\n",
        "  resolved_sentences.append(resolved_doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gga1Kr_wLr3a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "dict = {'sentence': resolved_sentences}  \n",
        "       \n",
        "df = pd.DataFrame(dict) \n",
        "    \n",
        "df.to_csv('chap-12.csv') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "BF6not4JL1Hj",
        "outputId": "3ab8aa29-e6ba-43c1-9be9-60704942cc9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "file_path = \"/content/13.txt\"\n",
        "f = open(file_path, \"r\")\n",
        "s = f.read()\n",
        "temp = sent_tokenize(s)\n",
        "\n",
        "resolved_sentences = []\n",
        "for s in temp:\n",
        "  s = s.replace('\\n', ' ')\n",
        "  doc = nlp(s)\n",
        "  resolved_doc = doc._.coref_resolved\n",
        "  resolved_sentences.append(resolved_doc)\n",
        "\n",
        "import pandas as pd\n",
        "dict = {'sentence': resolved_sentences}  \n",
        "       \n",
        "df = pd.DataFrame(dict) \n",
        "    \n",
        "df.to_csv('chap-13.csv') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sCboz6fuL9Tj",
        "outputId": "01ab4437-bc85-4f06-90ea-721a768ebe58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "file_path = \"/content/14.txt\"\n",
        "f = open(file_path, \"r\")\n",
        "s = f.read()\n",
        "temp = sent_tokenize(s)\n",
        "\n",
        "resolved_sentences = []\n",
        "for s in temp:\n",
        "  s = s.replace('\\n', ' ')\n",
        "  doc = nlp(s)\n",
        "  resolved_doc = doc._.coref_resolved\n",
        "  resolved_sentences.append(resolved_doc)\n",
        "\n",
        "import pandas as pd\n",
        "dict = {'sentence': resolved_sentences}  \n",
        "       \n",
        "df = pd.DataFrame(dict) \n",
        "    \n",
        "df.to_csv('chap-14.csv') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sLzRzth7MA3J",
        "outputId": "a474cca4-584c-4012-f83a-035a170ebbfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "file_path = \"/content/15.txt\"\n",
        "f = open(file_path, \"r\")\n",
        "s = f.read()\n",
        "temp = sent_tokenize(s)\n",
        "\n",
        "resolved_sentences = []\n",
        "for s in temp:\n",
        "  s = s.replace('\\n', ' ')\n",
        "  doc = nlp(s)\n",
        "  resolved_doc = doc._.coref_resolved\n",
        "  resolved_sentences.append(resolved_doc)\n",
        "\n",
        "import pandas as pd\n",
        "dict = {'sentence': resolved_sentences}  \n",
        "       \n",
        "df = pd.DataFrame(dict) \n",
        "    \n",
        "df.to_csv('chap-15.csv') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T8bssBX5MZUq",
        "outputId": "42a495c8-e512-4c88-d57e-a4c5b9e89287"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "file_path = \"/content/16.txt\"\n",
        "f = open(file_path, \"r\")\n",
        "s = f.read()\n",
        "temp = sent_tokenize(s)\n",
        "\n",
        "resolved_sentences = []\n",
        "for s in temp:\n",
        "  s = s.replace('\\n', ' ')\n",
        "  doc = nlp(s)\n",
        "  resolved_doc = doc._.coref_resolved\n",
        "  resolved_sentences.append(resolved_doc)\n",
        "\n",
        "import pandas as pd\n",
        "dict = {'sentence': resolved_sentences}  \n",
        "       \n",
        "df = pd.DataFrame(dict) \n",
        "    \n",
        "df.to_csv('chap-16.csv') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZtiGCD9qMcTa",
        "outputId": "d8353130-1bc9-4f73-ea0c-bce4287cc53e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "file_path = \"/content/17.txt\"\n",
        "f = open(file_path, \"r\")\n",
        "s = f.read()\n",
        "temp = sent_tokenize(s)\n",
        "\n",
        "resolved_sentences = []\n",
        "for s in temp:\n",
        "  s = s.replace('\\n', ' ')\n",
        "  doc = nlp(s)\n",
        "  resolved_doc = doc._.coref_resolved\n",
        "  resolved_sentences.append(resolved_doc)\n",
        "\n",
        "import pandas as pd\n",
        "dict = {'sentence': resolved_sentences}  \n",
        "       \n",
        "df = pd.DataFrame(dict) \n",
        "    \n",
        "df.to_csv('chap-17.csv') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1FjqsBDMeQm"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "file_path = \"/content/18.txt\"\n",
        "f = open(file_path, \"r\")\n",
        "s = f.read()\n",
        "temp = sent_tokenize(s)\n",
        "\n",
        "resolved_sentences = []\n",
        "for s in temp:\n",
        "  s = s.replace('\\n', ' ')\n",
        "  doc = nlp(s)\n",
        "  resolved_doc = doc._.coref_resolved\n",
        "  resolved_sentences.append(resolved_doc)\n",
        "\n",
        "import pandas as pd\n",
        "dict = {'sentence': resolved_sentences}  \n",
        "       \n",
        "df = pd.DataFrame(dict) \n",
        "    \n",
        "df.to_csv('chap-18.csv') "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CoReference.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}