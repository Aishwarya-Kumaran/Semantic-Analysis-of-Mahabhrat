{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "relationship tagging-.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn-crfsuite\n",
        "import pickle\n",
        "# filename = '/content/drive/MyDrive/CRF_model/model6.sav'\n",
        "filename = '/content/drive/MyDrive/CRF_model/model6.sav'\n",
        "crf_model=pickle.load(open(filename,'rb'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXRSirLfbjSx",
        "outputId": "18249f06-6c24-47a3-bf79-7bd4abe35a85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn-crfsuite\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (1.15.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (4.64.0)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "  Downloading python_crfsuite-0.9.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (965 kB)\n",
            "\u001b[K     |████████████████████████████████| 965 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (0.8.9)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.8 sklearn-crfsuite-0.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install flair"
      ],
      "metadata": {
        "id": "tZ1qnELdzEv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KPuXC6Ebs5j",
        "outputId": "f17bbe49-fbf0-400b-f077-5c89ee01f6f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from flair.models import SequenceTagger\n",
        "# from flair.data import Sentence\n",
        "# tagger = SequenceTagger.load('pos')\n",
        "# words = []\n",
        "# POS = []\n",
        "# sentence = Sentence(\"Arjuna and Draupadi lived in Hastinapura\")\n",
        "# tagger.predict(sentence)\n",
        "# pos_tags = sentence.to_tagged_string()\n",
        "# print(pos_tags)\n",
        "# pos_tags_split = pos_tags.split()\n",
        "# print(pos_tags_split)\n",
        "# for i in range(0, len(pos_tags_split), 2):\n",
        "#   words.append([pos_tags_split[i],pos_tags_split[i+1]])\n",
        "\n",
        "# print(words)"
      ],
      "metadata": {
        "id": "JUPRsxSfbCt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')  \n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from nltk import pos_tag\n",
        "from nltk import RegexpParser\n",
        "# sentence = \"Barack, Obama was born in Hawaii\"\n",
        "\n",
        "# text = word_tokenize(sentence)\n",
        "# words = pos_tag(text)\n",
        "# print(\"After Token:\",words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X897UhMe2Lpd",
        "outputId": "e11e8311-a803-4831-9061-d0a0ad04f90f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# s = words\n",
        "# print(s)\n",
        "def word2features(sent, i):\n",
        "    word = sent[i][0]\n",
        "    postag = sent[i][1]\n",
        "\n",
        "    features = {\n",
        "        'bias': 1.0,\n",
        "        'word.lower()': word.lower(),\n",
        "        'word[-3:]': word[-3:],\n",
        "        'word[-2:]': word[-2:],\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit(),\n",
        "        'postag': postag,\n",
        "        'postag[:2]': postag[:2],\n",
        "    }\n",
        "    # other than first word \n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        postag1 = sent[i-1][1]\n",
        "        features.update({\n",
        "            '-1:word.lower()': word1.lower(),\n",
        "            '-1:word.istitle()': word1.istitle(),\n",
        "            '-1:word.isupper()': word1.isupper(),\n",
        "            '-1:postag': postag1,\n",
        "            '-1:postag[:2]': postag1[:2],\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1][0]\n",
        "        postag1 = sent[i+1][1]\n",
        "        features.update({\n",
        "            '+1:word.lower()': word1.lower(),\n",
        "            '+1:word.istitle()': word1.istitle(),\n",
        "            '+1:word.isupper()': word1.isupper(),\n",
        "            '+1:postag': postag1,\n",
        "            '+1:postag[:2]': postag1[:2],\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return [label for token, postag, label in sent]\n",
        "\n",
        "def sent2tokens(sent):\n",
        "    return [token for token, postag, label in sent]\n",
        "\n"
      ],
      "metadata": {
        "id": "-zloyDQqbbyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WFt-COHycb0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xF0jfSLEz2Pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "qJ3okSRz9cmU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ebb8a14-3143-443e-ecde-4f2e19813c4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# document = 'Barack Obama was born in Hawaii. Richard Manning wrote this sentence.'\n",
        "# dict = {'subject': 'Barack Obama', 'relation': 'was', 'object': 'born'}\n",
        "# document_name = 'Barack Obama was born in Hawaii. Richard Manning wrote this sentence.'\n",
        "tokens = []\n",
        "relations = []"
      ],
      "metadata": {
        "id": "IjUdfJm7Y8Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_dict = []"
      ],
      "metadata": {
        "id": "fxdblJMpcO8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/tag_crf.csv')\n",
        "\n",
        "# df = df[:2]\n",
        "for index, row in df.iterrows():\n",
        "    tokens = []\n",
        "    relations = []\n",
        "    document = row['document']    \n",
        "    text = row['subject']\n",
        "    tok_temp = {}\n",
        "    relation_temp={}\n",
        "\n",
        "    #ner tag\n",
        "    words = pos_tag(document)\n",
        "    # print(\"After Token:\",words)\n",
        "    s = words\n",
        "    ip = sent2features(s)\n",
        "    # print([ip])\n",
        "    pred = crf_model.predict([ip])\n",
        "    # print(pred)\n",
        "\n",
        "    # text = df['subject']\n",
        "    start = document.find(text)\n",
        "    end = start + len(text) \n",
        "    # print(start)\n",
        "    # print(end)\n",
        "    # print(text)\n",
        "    doc_split = word_tokenize(document)\n",
        "    text_split = word_tokenize(text)\n",
        "    token_start = [i for i, e in enumerate(doc_split) if e == text_split[0]]\n",
        "    if len(token_start) == 0:\n",
        "      continue\n",
        "    token_start = token_start[0]\n",
        "    # token_start = doc_split.index(text_split[0])\n",
        "    token_end = [i for i, e in enumerate(doc_split) if e == text_split[-1]]\n",
        "    if len(token_end) == 0:\n",
        "      continue\n",
        "    token_end = token_end[0]\n",
        "    if token_start == -1 or token_end == -1:\n",
        "      continue\n",
        "    if start == -1 or end == -1:\n",
        "      continue\n",
        "    print(token_start)\n",
        "    print(token_end)\n",
        "    tok_temp['text'] = text\n",
        "    tok_temp['start'] = start\n",
        "    tok_temp['end'] = end\n",
        "    tok_temp['token_start'] = relation_temp['head'] = token_start\n",
        "    tok_temp['token_end'] = token_end\n",
        "\n",
        "\n",
        "    tok_temp['entityLabel'] = pred[0][token_start]\n",
        "    tokens.append(tok_temp)\n",
        "\n",
        "    tok_temp = {}\n",
        "    text = row['object']\n",
        "    start = document.find(text)\n",
        "    end = start + len(text)\n",
        "    print(start)\n",
        "    print(end)\n",
        "    print(text)\n",
        "    print(document)\n",
        "    doc_split = word_tokenize(document)\n",
        "    text_split = word_tokenize(text)\n",
        "    print(text_split[0])\n",
        "    token_end = 0\n",
        "    token_start = [i for i, e in enumerate(doc_split) if e == text_split[0]]\n",
        "    if len(token_start) == 0:\n",
        "      continue\n",
        "    token_start = token_start[0]\n",
        "    # token_start = doc_split.index(text_split[0])\n",
        "    token_end = 0\n",
        "    token_end = [i for i, e in enumerate(doc_split) if e == text_split[-1]]\n",
        "    if len(token_end) == 0:\n",
        "      continue\n",
        "    token_end = token_end[0]\n",
        "    if token_start == -1 or token_end == -1:\n",
        "      continue\n",
        "    if start == -1 or end == -1:\n",
        "      continue\n",
        "    print(token_start)\n",
        "    print(token_end)\n",
        "    tok_temp['text'] = text\n",
        "    tok_temp['start'] = start\n",
        "    tok_temp['end'] = end\n",
        "    tok_temp['token_start'] = relation_temp['child']= token_start\n",
        "    tok_temp['token_end'] = token_end\n",
        "    tok_temp['entityLabel'] = pred[0][token_start]\n",
        "    tokens.append(tok_temp)\n",
        "\n",
        "    relation_temp['relationLabel'] = row['relation']\n",
        "\n",
        "    relations.append(relation_temp)\n",
        "    print(tokens)\n",
        "    print(relations)\n",
        "    temp = {}\n",
        "    temp[\"document_name\"] = document\n",
        "    temp[\"document\"] = document\n",
        "    temp[\"tokens\"] = tokens\n",
        "    temp[\"relations\"] = relations\n",
        "    final_dict.append(temp)"
      ],
      "metadata": {
        "id": "S2EWNyea0KVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(tokens)\n",
        "# print(relations)"
      ],
      "metadata": {
        "id": "-Oi_cGouJFjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# temp = {}\n",
        "# temp[\"document_name\"] = document_name\n",
        "# temp[\"document\"] = document\n",
        "# temp[\"tokens\"] = tokens\n",
        "# temp[\"relations\"] = relations"
      ],
      "metadata": {
        "id": "9RtXfEZ1LiMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(temp)"
      ],
      "metadata": {
        "id": "dfngE6egMHWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(final_dict))"
      ],
      "metadata": {
        "id": "W9gzDa2cFJUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    \n",
        "with open(\"tagged_data_2.json\", \"w\") as outfile:\n",
        "    json.dump(final_dict, outfile)"
      ],
      "metadata": {
        "id": "2x8byd3y3niC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "json_object = json.dumps(final_dict, indent = 4) \n",
        "print(json_object)"
      ],
      "metadata": {
        "id": "RcAVnJy9NFp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"train_data.txt\", \"w\") as output:\n",
        "    output.write(str(final_dict))"
      ],
      "metadata": {
        "id": "XbqURXcXv_uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "w9RImi0Trf-D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}