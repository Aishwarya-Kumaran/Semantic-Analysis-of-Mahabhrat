{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "conversion_code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6uqYVOUxhCc",
        "outputId": "80640fee-af67-4b02-a4c1-fd5660e6d4da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spacy-nightly\n",
            "  Downloading spacy_nightly-3.0.0rc5-cp37-cp37m-manylinux2014_x86_64.whl (12.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.7 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy-nightly) (2.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy-nightly) (0.9.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy-nightly) (1.0.7)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy-nightly) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy-nightly) (4.2.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy-nightly) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy-nightly) (21.3)\n",
            "Collecting pathy\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.3 MB/s \n",
            "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.0.dev0\n",
            "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy-nightly) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy-nightly) (57.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy-nightly) (3.0.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy-nightly) (2.11.3)\n",
            "Collecting srsly<3.0.0,>=2.3.0\n",
            "  Downloading srsly-2.4.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (457 kB)\n",
            "\u001b[K     |████████████████████████████████| 457 kB 49.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from spacy-nightly) (4.11.3)\n",
            "Collecting catalogue<2.1.0,>=2.0.1\n",
            "  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
            "Collecting thinc<8.1.0,>=8.0.0\n",
            "  Downloading thinc-8.0.16-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (660 kB)\n",
            "\u001b[K     |████████████████████████████████| 660 kB 64.0 MB/s \n",
            "\u001b[?25hCollecting typer<0.4.0,>=0.3.0\n",
            "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
            "Collecting pydantic<1.8.0,>=1.7.1\n",
            "  Downloading pydantic-1.7.4-cp37-cp37m-manylinux2014_x86_64.whl (9.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1 MB 45.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy-nightly) (1.21.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.1->spacy-nightly) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy-nightly) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy-nightly) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy-nightly) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy-nightly) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy-nightly) (2021.10.8)\n",
            "Collecting typing-extensions>=3.7.4\n",
            "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy-nightly) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy-nightly) (2.0.1)\n",
            "Collecting smart-open<6.0.0,>=5.0.0\n",
            "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: typing-extensions, catalogue, typer, srsly, smart-open, pydantic, thinc, spacy-legacy, pathy, spacy-nightly\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.2.0\n",
            "    Uninstalling typing-extensions-4.2.0:\n",
            "      Successfully uninstalled typing-extensions-4.2.0\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 6.0.0\n",
            "    Uninstalling smart-open-6.0.0:\n",
            "      Successfully uninstalled smart-open-6.0.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0+zzzcolab20220506162203 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "spacy 2.2.4 requires catalogue<1.1.0,>=0.0.7, but you have catalogue 2.0.7 which is incompatible.\n",
            "spacy 2.2.4 requires srsly<1.1.0,>=1.0.2, but you have srsly 2.4.3 which is incompatible.\n",
            "spacy 2.2.4 requires thinc==7.4.0, but you have thinc 8.0.16 which is incompatible.\u001b[0m\n",
            "Successfully installed catalogue-2.0.7 pathy-0.6.1 pydantic-1.7.4 smart-open-5.2.1 spacy-legacy-3.0.9 spacy-nightly-3.0.0rc5 srsly-2.4.3 thinc-8.0.16 typer-0.3.2 typing-extensions-3.10.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -U spacy-nightly --pre"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!python -m spacy project clone tutorials/rel_component"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        },
        "id": "GN1UJlJI2LDF",
        "outputId": "854d571b-6b55-439e-e41b-85da2f20e090"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-22.1.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-62.3.2-py3-none-any.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 51.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (0.37.1)\n",
            "Installing collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0+zzzcolab20220506162203 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "spacy 2.2.4 requires catalogue<1.1.0,>=0.0.7, but you have catalogue 2.0.7 which is incompatible.\n",
            "spacy 2.2.4 requires srsly<1.1.0,>=1.0.2, but you have srsly 2.4.3 which is incompatible.\n",
            "spacy 2.2.4 requires thinc==7.4.0, but you have thinc 8.0.16 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed pip-22.1.1 setuptools-62.3.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Cloned 'tutorials/rel_component' from explosion/projects\u001b[0m\n",
            "/content/rel_component\n",
            "\u001b[38;5;2m✔ Your project is now ready!\u001b[0m\n",
            "To fetch the assets, run:\n",
            "python -m spacy project assets /content/rel_component\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python binary_converter.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZ19aguK2PBE",
        "outputId": "3708fd4f-3b3f-4abe-db9c-bf740fa7fd3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[None, None]\n",
            "Traceback (most recent call last):\n",
            "  File \"binary_converter.py\", line 123, in <module>\n",
            "    main(ann, train_file, dev_file, test_file)\n",
            "  File \"binary_converter.py\", line 70, in main\n",
            "    doc.ents = entities\n",
            "  File \"spacy/tokens/doc.pyx\", line 725, in spacy.tokens.doc.Doc.ents.__set__\n",
            "  File \"spacy/tokens/doc.pyx\", line 1703, in spacy.tokens.doc.get_entity_info\n",
            "TypeError: object of type 'NoneType' has no len()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# import typer\n",
        "from pathlib import Path\n",
        "\n",
        "from spacy.tokens import Span, DocBin, Doc\n",
        "from spacy.vocab import Vocab\n",
        "from wasabi import Printer\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.en import English\n",
        "from spacy.util import compile_infix_regex\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "# Create a blank Tokenizer with just the English vocab\n",
        "\n",
        "msg = Printer()\n",
        "\n",
        "SYMM_LABELS = [\"Binds\"]\n",
        "MAP_LABELS = { \"child_of\" : \"child_of\", \"king_of\" : \"king_of\", \"born_in\" : \"born_in\", \"master_in\" : \"master_in\", \"killed\" : \"killed\", \"lived_in\" : \"lived_in\", \"happened_in\" : \"happened_in\", \"spouse_of\" : \"spouse_of\", \n",
        "\"friend_of\" : \"friend_of\", \"leading\" : \"leading\", \"sibling_of\" : \"sibling_of\", \"is_a_part_of\" : \"is_a_part_of\", \"guardian_of\" : \"guardian_of\", \"killed\" : \"killed\" }\n",
        "# MAP_LABELS = {\n",
        "#     \"DEGREE_IN\": \"DEGREE_IN\",\n",
        "#     \"EXPERIENCE_IN\": \"EXPERIENCE_IN\"\n",
        "# }\n",
        "ann = \"/content/testv3.txt\"\n",
        "# ann = \"/content/relations_training.txt\"\n",
        "train_file='/content/rel_component/data/relations_training.spacy'\n",
        "dev_file='/content/rel_component/data/relations_dev.spacy'\n",
        "test_file='/content/rel_component/data/relations_test.spacy'\n",
        "\n",
        "def main(json_loc: Path, train_file: Path, dev_file: Path, test_file: Path):\n",
        "    \"\"\"Creating the corpus from the Prodigy annotations.\"\"\"\n",
        "    Doc.set_extension(\"rel\", default={},force=True)\n",
        "    vocab = Vocab()\n",
        "\n",
        "    docs = {\"train\": [], \"dev\": [], \"test\": [], \"total\": []}\n",
        "    ids = {\"train\": set(), \"dev\": set(), \"test\": set(), \"total\":set()}\n",
        "    count_all = {\"train\": 0, \"dev\": 0, \"test\": 0,\"total\": 0}\n",
        "    count_pos = {\"train\": 0, \"dev\": 0, \"test\": 0,\"total\": 0}\n",
        "    i = 0\n",
        "    with open(json_loc, encoding=\"utf8\") as jsonfile:\n",
        "        file = json.load(jsonfile)\n",
        "        for example in file:\n",
        "            print(i)\n",
        "            i+=1\n",
        "            span_starts = set()\n",
        "            neg = 0\n",
        "            pos = 0\n",
        "                    # Parse the tokens\n",
        "            tokens=nlp(example[\"document\"])    \n",
        "            print(tokens)\n",
        "            spaces=[]\n",
        "            spaces = [True if tok.whitespace_ else False for tok in tokens]\n",
        "            words = [t.text for t in tokens]\n",
        "            doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
        "            print(doc)\n",
        "\n",
        "            # if(example['tokens'][0]['start'] == -1 or example['tokens'][1]['start'] == -1):\n",
        "            #   continue\n",
        "            \n",
        "            if(example['tokens'][0]['start']<=example['tokens'][1]['start']):\n",
        "              if(example['tokens'][0]['end']>example['tokens'][1]['start']):\n",
        "                continue\n",
        "\n",
        "            if(example['tokens'][1]['start']<=example['tokens'][0]['start']):\n",
        "              if(example['tokens'][1]['end']>example['tokens'][0]['start']):\n",
        "                continue\n",
        "                      \n",
        "\n",
        "\n",
        "            # Parse the GGP entities\n",
        "            print('spans')\n",
        "            spans = example[\"tokens\"]\n",
        "            print(spans)\n",
        "            entities = []\n",
        "            span_end_to_start = {}\n",
        "            for span in spans:\n",
        "                print(span)\n",
        "                entity = doc.char_span(\n",
        "                     span[\"start\"], span[\"end\"], label=span[\"entityLabel\"]\n",
        "                 )\n",
        "                print(entity)\n",
        "                if(entity == None):\n",
        "                  x = 0\n",
        "                  break\n",
        "                # if(!type(entity)):\n",
        "                #   continue\n",
        "                span_end_to_start[span[\"token_start\"]] = span[\"token_start\"]\n",
        "                #print(span_end_to_start)\n",
        "                entities.append(entity)\n",
        "                span_starts.add(span[\"token_start\"])\n",
        "            if(len(entities) != 2):\n",
        "              continue\n",
        "            print(entities)\n",
        "\n",
        "            doc.ents = entities\n",
        "\n",
        "            # Parse the relations\n",
        "            rels = {}\n",
        "            for x1 in span_starts:\n",
        "                for x2 in span_starts:\n",
        "                    rels[(x1, x2)] = {}\n",
        "                    #print(rels)\n",
        "            relations = example[\"relations\"]\n",
        "            #print(len(relations))\n",
        "            for relation in relations:\n",
        "                # the 'head' and 'child' annotations refer to the end token in the span\n",
        "                # but we want the first token\n",
        "                start = span_end_to_start[relation[\"head\"]]\n",
        "                end = span_end_to_start[relation[\"child\"]]\n",
        "                label = relation[\"relationLabel\"]\n",
        "                #print(rels[(start, end)])\n",
        "                #print(label)\n",
        "                #label = MAP_LABELS[label]\n",
        "                if label not in rels[(start, end)]:\n",
        "                    rels[(start, end)][label] = 1.0\n",
        "                    pos += 1\n",
        "                    #print(pos)\n",
        "                    #print(rels[(start, end)])\n",
        "\n",
        "            # The annotation is complete, so fill in zero's where the data is missing\n",
        "            for x1 in span_starts:\n",
        "                for x2 in span_starts:\n",
        "                    for label in MAP_LABELS.values():\n",
        "                        if label not in rels[(x1, x2)]:\n",
        "                            neg += 1\n",
        "                            rels[(x1, x2)][label] = 0.0\n",
        "\n",
        "                            #print(rels[(x1, x2)])\n",
        "            doc._.rel = rels\n",
        "            #print(doc._.rel)\n",
        "\n",
        "            # only keeping documents with at least 1 positive case\n",
        "            if pos > 0:\n",
        "                    docs[\"total\"].append(doc)\n",
        "                    count_pos[\"total\"] += pos\n",
        "                    count_all[\"total\"] += pos + neg\n",
        "\n",
        "                    \n",
        "                    \n",
        "    #print(len(docs[\"total\"]))\n",
        "    docbin = DocBin(docs=docs[\"total\"], store_user_data=True)\n",
        "    docbin.to_disk(test_file)\n",
        "    msg.info(\n",
        "        f\"{len(docs['total'])} training sentences\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "mkwBVf3y44qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(ann, train_file, dev_file, test_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHx1ZHRQ492b",
        "outputId": "ddf4985e-adb3-4953-b851-829571659128"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "This mighty-armed one, having fought Dhananjaya, the son of Pritha, and having reduced This mighty-armed one to great straits, was at last slain by This mighty-armed one antagonist.\n",
            "This mighty-armed one, having fought Dhananjaya, the son of Pritha, and having reduced This mighty-armed one to great straits, was at last slain by This mighty-armed one antagonist.\n",
            "spans\n",
            "[{'text': 'having fought Dhananjaya', 'start': 23, 'end': 47, 'token_start': 4, 'token_end': 6, 'entityLabel': 'O'}, {'text': 'Pritha', 'start': 60, 'end': 66, 'token_start': 11, 'token_end': 11, 'entityLabel': 'O'}]\n",
            "{'text': 'having fought Dhananjaya', 'start': 23, 'end': 47, 'token_start': 4, 'token_end': 6, 'entityLabel': 'O'}\n",
            "having fought Dhananjaya\n",
            "{'text': 'Pritha', 'start': 60, 'end': 66, 'token_start': 11, 'token_end': 11, 'entityLabel': 'O'}\n",
            "Pritha\n",
            "[having fought Dhananjaya, Pritha]\n",
            "1\n",
            "Behold, O Keshava, the two brothers of Avanti, Vinda and Anuvinda, lying there on the field, like two blossoming shala trees in the spring overthrown by the tempest.\n",
            "Behold, O Keshava, the two brothers of Avanti, Vinda and Anuvinda, lying there on the field, like two blossoming shala trees in the spring overthrown by the tempest.\n",
            "spans\n",
            "[{'text': 'Behold', 'start': 0, 'end': 6, 'token_start': 0, 'token_end': 0, 'entityLabel': 'O'}, {'text': 'Avanti', 'start': 39, 'end': 45, 'token_start': 9, 'token_end': 9, 'entityLabel': 'O'}]\n",
            "{'text': 'Behold', 'start': 0, 'end': 6, 'token_start': 0, 'token_end': 0, 'entityLabel': 'O'}\n",
            "Behold\n",
            "{'text': 'Avanti', 'start': 39, 'end': 45, 'token_start': 9, 'token_end': 9, 'entityLabel': 'O'}\n",
            "Avanti\n",
            "[Behold, Avanti]\n",
            "2\n",
            "Vaishampayana continued, \"Thus addressed, Kunti's son Yudhishthira of great wisdom commanded Sudharma (the priest of the Kauravas) and Dhaumya, and Sanjaya of the Suta order, and Vidura of great wisdom, and Yuyutsu of Kurus race, and all his servants headed by Indrasena, and all the other Sutas that were with his, saying, 'Cause the funeral rites of the slain, numbering by thousands, to be duly performed, so that nobody may perish for want of persons to take care of persons!'\n",
            "Vaishampayana continued, \"Thus addressed, Kunti's son Yudhishthira of great wisdom commanded Sudharma (the priest of the Kauravas) and Dhaumya, and Sanjaya of the Suta order, and Vidura of great wisdom, and Yuyutsu of Kurus race, and all his servants headed by Indrasena, and all the other Sutas that were with his, saying, 'Cause the funeral rites of the slain, numbering by thousands, to be duly performed, so that nobody may perish for want of persons to take care of persons!'\n",
            "spans\n",
            "[{'text': 'Kunti', 'start': 42, 'end': 47, 'token_start': 7, 'token_end': 7, 'entityLabel': 'O'}, {'text': 'Yudhishthira', 'start': 54, 'end': 66, 'token_start': 10, 'token_end': 10, 'entityLabel': 'O'}]\n",
            "{'text': 'Kunti', 'start': 42, 'end': 47, 'token_start': 7, 'token_end': 7, 'entityLabel': 'O'}\n",
            "Kunti\n",
            "{'text': 'Yudhishthira', 'start': 54, 'end': 66, 'token_start': 10, 'token_end': 10, 'entityLabel': 'O'}\n",
            "Yudhishthira\n",
            "[Kunti, Yudhishthira]\n",
            "\u001b[38;5;4mℹ 3 training sentences\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IBXCoQ1zM6J0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}